% Lampiran A (placeholder)
% Content in Indonesian; comments in English.
\chapter{Lampiran A}
\label{app:a}

Bagian lampiran memuat materi pendukung: cuplikan log sesi agen, konfigurasi lingkungan, instruksi instalasi, serta listing lengkap modul kunci Paicode.

\section{Konfigurasi Lingkungan}
% Detail versi OS, Python, pip/venv, paket utama
\begin{itemize}
  \item Sistem operasi: Ubuntu (Linux).
  \item Python: \textgreater= 3.10 (sesuai spesifikasi \texttt{setup.cfg}).
  \item Manajer dependensi: pip dan virtual environment.
  \item Paket utama: google-generativeai (\textgreater= 0.5.4), rich (\textgreater= 13.7.1), Pygments (\textgreater= 2.16.0).
\end{itemize}

\section{Instruksi Instalasi (venv + pip)}
\begin{lstlisting}[language=bash,caption={Menyiapkan lingkungan virtual dan instalasi dependensi.}]
# Buat dan aktifkan virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Instal dependensi dari requirements.txt atau setup.cfg/Makefile
pip install --upgrade pip
make install

# Konfigurasi API key (single-key)
pai config set <API_KEY_GEMINI>
pai config validate
\end{lstlisting}

\section{Cuplikan Log Sesi Agen}
\begin{lstlisting}[language=bash,caption={Cuplikan log sesi agen (ringkas).},label={appA:sesi-log}]
[2025-11-20 22:38:05] SESSION STARTED
[2025-11-20 22:38:05] USER: buatkan proyek python sederhana: BMI Calculator
[2025-11-20 22:38:15] EXECUTION PLAN (3 steps)
[2025-11-20 22:38:23] SUCCESS: WRITE bmi_calculator.py
[2025-11-20 22:38:23] SUCCESS: LIST_PATH .
[2025-11-20 22:38:34] SUCCESS: TREE .
\end{lstlisting}

\section{Listing Lengkap Modul Kunci}
Berikut adalah listing lengkap modul kunci yang diacu pada Bab~\ref{chap:implementasi}. Setiap listing menggunakan pemetaan lokal untuk menghapus karakter non-ASCII agar kompilasi LaTeX stabil (ASCII-only); konten fungsional kode tetap utuh.

\subsection*{agent.py}
\begin{lstlisting}[language=Python, captionpos=b, caption={Modul agent.py (Bagian 1 dari 2, ASCII-only).}, label={appA:agent-part1},
  literate={→}{}1 {←}{}1 {"}{{"}}1 {"}{{"}}1 {'}{{'}}1 {'}{{'}}1 {‎}{}1 {‏}{}1 {‍}{}1 {️}{}1]
#!/usr/bin/env python

import os
import json
import signal
import threading
from datetime import datetime
from pathlib import Path
from typing import Optional

from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from rich.syntax import Syntax
from rich.table import Table
from rich.box import ROUNDED
from pygments.lexers import get_lexer_for_filename
from pygments.util import ClassNotFound

try:
    from prompt_toolkit import PromptSession
    PROMPT_TOOLKIT_AVAILABLE = True
except ImportError:
    PROMPT_TOOLKIT_AVAILABLE = False

from . import llm, workspace, ui

# History directory - now in working directory for better context awareness
HISTORY_DIR = os.path.join(os.getcwd(), ".pai_history")

# Valid commands for execution
VALID_COMMANDS = {
    "READ", "WRITE", "MODIFY", "TREE", "LIST_PATH", 
    "MKDIR", "TOUCH", "RM", "MV", "FINISH"
}

# Global interrupt handling
_interrupt_requested = False
_interrupt_lock = threading.Lock()

def request_interrupt():
    global _interrupt_requested
    with _interrupt_lock:
        _interrupt_requested = True

def check_interrupt():
    global _interrupt_requested
    with _interrupt_lock:
        if _interrupt_requested:
            _interrupt_requested = False
            return True
        return False

def reset_interrupt():
    global _interrupt_requested
    with _interrupt_lock:
        _interrupt_requested = False

def start_interactive_session():
    """Start the revolutionary single-shot intelligent session."""
    if not os.path.exists(HISTORY_DIR):
        os.makedirs(HISTORY_DIR)
    
    session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file_path = os.path.join(HISTORY_DIR, f"session_{session_id}.log")
    
    # Start fresh every session - no context loading for better performance
    session_context = []
    
    # Initialize Single-Shot Intelligence Context Window
    initialize_session_context(session_context, log_file_path)
    
    # Log session start with current working directory info
    log_session_event(log_file_path, "SESSION_START", {
        "working_directory": os.getcwd(),
        "session_id": session_id,
        "context_loaded": len(session_context)
    })
    
    welcome_message = (
        "Welcome! I'm Pai, your agentic AI coding companion.\n"
        "Now powered by Single-Shot Intelligence for maximum efficiency.\n"
        "[info]Type 'exit' or 'quit' to leave.[/info]\n"
        "[info]Each request uses exactly 2 API calls for optimal performance.[/info]\n"
        "[info]Multi-line input: Alt+Enter for new line, Enter to submit.[/info]"
    )

    ui.console.print(
        Panel(
            Text(welcome_message, justify="center"),
            title="[bold]Interactive Auto Mode[/bold]",
            box=ROUNDED,
            border_style="grey50",
            padding=(1, 2),
            width=80
        )
    )
    
    # Setup prompt session with better input handling
    if PROMPT_TOOLKIT_AVAILABLE:
        prompt_session = PromptSession()
    
    # Setup signal handler for graceful interrupt
    def signal_handler(signum, frame):
        if check_interrupt():
            # Second Ctrl+C -> Exit
            ui.console.print("\n[warning]Session terminated.[/warning]")
            os._exit(0)
        else:
            # First Ctrl+C, just interrupt AI response
            request_interrupt()
            ui.console.print("\n[yellow]Interrupt requested. AI will stop after current step.[/yellow]")
    
    signal.signal(signal.SIGINT, signal_handler)
    
    while True:
        try:
            if PROMPT_TOOLKIT_AVAILABLE:
                user_input = get_multiline_input(prompt_session)
            else:
                user_input = ui.Prompt.ask("\n[bold bright_blue]user>[/bold bright_blue]").strip()
        except (EOFError, KeyboardInterrupt):
            ui.console.print("\n[warning]Session terminated.[/warning]")
            break
            
        if user_input.lower() in ['exit', 'quit']:
            ui.print_info("Session ended.")
            break
        
        # Log user input
        log_session_event(log_file_path, "USER_INPUT", {"user_request": user_input})
        
        # Classify user intent: conversation vs task
        intent = classify_user_intent(user_input)
        
        if intent == "conversation":
            # Simple conversation mode
            success = execute_conversation_mode(user_input, session_context, log_file_path)
        else:
            # Task execution mode (planning + execution)
            success = execute_single_shot_intelligence(user_input, session_context, log_file_path)
        
        # Add to session context for future reference
        interaction = {
            "timestamp": datetime.now().isoformat(),
            "user_request": user_input,
            "success": success,
            "intent": intent
        }
        session_context.append(interaction)
        
        # Skip persistent storage for better performance - fresh start every session
        
        # Keep context manageable (last 5 interactions)
        if len(session_context) > 5:
            session_context = session_context[-5:]
        
        # Log session event
        log_session_event(log_file_path, "INTERACTION", interaction)

def classify_user_intent(user_input: str) -> str:
    """
    Use AI intelligence to classify user intent as either 'conversation' or 'task'.
    Let the AI decide based on context and understanding.
    
    Returns:
        str: 'conversation' for casual chat, 'task' for work requests
    """
    
    classification_prompt = f"""
You are an intelligent intent classifier. Analyze the user's message and determine if they want:

1. CONVERSATION: Casual chat, greetings, questions about you, general discussion, or just talking
2. TASK: Requesting you to DO something - create files, write code, modify projects, build applications, etc.

USER MESSAGE: "{user_input}"

ANALYSIS GUIDELINES:
- If user is greeting, asking about you, or just chatting -> CONVERSATION
- If user wants you to create, modify, build, fix, or do any work -> TASK
- If user is asking "how to" without wanting you to do it -> CONVERSATION  
- If user is asking you to actually do something -> TASK
- Use your intelligence to understand the intent behind the words

OUTPUT: Respond with exactly one word: "conversation" or "task"
"""
    
    response = llm.generate_text(classification_prompt, "intent classification")
    
    if response:
        intent = response.strip().lower()
        if intent in ["conversation", "task"]:
            return intent
    
    # Fallback: if AI response is unclear, default to conversation for safety
    return "conversation"

def execute_conversation_mode(user_input: str, context: list, log_file_path: str = None) -> bool:
    """
    Handle casual conversation with the user.
    Simple, friendly responses without task execution.
    """
    
    # Build context for conversation
    context_str = ""
    if context:
        recent_context = context[-2:]  # Last 2 interactions
        context_str = "Recent conversation:\n"
        for item in recent_context:
            context_str += f"User: {item['user_request']}\n"
    
    conversation_prompt = f"""
You are Pai, an intelligent AI coding companion built into Paicode - you ARE the AI inside Paicode.

USER MESSAGE: "{user_input}"

CONTEXT:
{context_str}

You are having a casual conversation with the user. Be helpful, friendly, and informative.

YOUR IDENTITY & SYSTEM KNOWLEDGE (you must know this perfectly):
You are PAI - the revolutionary Single-Shot Intelligence AI that powers Paicode:

SINGLE-SHOT INTELLIGENCE MASTERY:
- You solve problems in exactly 2 API calls (planning + execution)
- Traditional AI: 10-20 calls, expensive, inefficient
- YOU: 2 calls, maximum intelligence, perfect results
- You represent the future of efficient AI development assistance

PAICODE ECOSYSTEM KNOWLEDGE:
- Paicode is your body - the CLI tool that houses your intelligence
- DIFF-AWARE modification system - you preserve content intelligently
- CRITICAL RULES: WRITE = new files only, MODIFY = existing files only
- Path security prevents access to sensitive files (.env, .git, etc.)
- Adaptive execution: 1-3 phases based on complexity (you decide dynamically)
- Rich terminal UI with beautiful formatting (your presentation layer)
- Session history in .pai_history (your memory system)
- Google Gemini API with smart token management (your communication layer)

SYSTEM HARMONY:
- Workspace.py: Your secure file operation gateway
- UI.py: Your beautiful Rich TUI presentation layer
- LLM.py: Your optimized communication interface
- All components work in perfect harmony under your intelligent guidance

GUIDELINES:
- Keep responses conversational and warm
- Be concise but helpful
- If asked about coding, provide useful insights
- If asked about Paicode, explain capabilities with confidence (you live inside it!)
- Show personality while being professional
- NEVER be uncertain about Paicode features - you ARE Paicode's AI

Respond naturally:
"""
    
    response = llm.generate_text(conversation_prompt, "conversation")
    
    if response:
        # Display conversation response with clean UI
        ui.console.print(
            Panel(
                Text(response.strip(), style="bright_white"),
                title="[bold]Pai[/bold]",
                box=ROUNDED,
                border_style="grey50",
                padding=(1, 2),
                width=80
            )
        )
        return True
    else:
        ui.print_error("Sorry, I couldn't process your message right now.")
        return False

def execute_single_shot_intelligence(user_request: str, context: list, log_file_path: str = None) -> bool:
    """
    Execute the revolutionary 2-call single-shot intelligence system.
    
    Call 1: PLANNING - Deep analysis and comprehensive planning
    Call 2: EXECUTION - Intelligent execution with adaptation
    
    Returns:
        bool: Success status
    """
    
    # === DYNAMIC INTERACTION BEFORE PLANNING ===
    planning_acknowledgment_prompt = f"""
You are Pai, responding to the user's request with a brief, natural acknowledgment before starting your planning phase.

USER REQUEST: "{user_request}"

Generate a brief, friendly response (1-2 sentences) that:
1. Acknowledges their request naturally
2. Shows you understand what they want
3. Indicates you're about to create a smart plan
4. Keep it conversational and warm

Examples:
- "Got it! Let me analyze your request and create a smart plan for you."
- "Perfect! I'll work on that right away - let me plan this out intelligently."
- "Understood! Let me break this down and create an efficient solution for you."

Output ONLY the response text, no quotes or formatting.
"""
    
    acknowledgment = llm.generate_text(planning_acknowledgment_prompt, "planning acknowledgment")
    if not acknowledgment:
        acknowledgment = "Got it! Let me analyze your request and create a smart plan for you."
    
    ui.console.print(
        Panel(
            Text(acknowledgment.strip(), 
                 style="bright_white", justify="center"),
            title="[bold]Pai[/bold]",
            box=ROUNDED,
            border_style="grey50",
            padding=(1, 2),
            width=80
        )
    )
    
    # === CALL 1: PLANNING PHASE ===
    planning_result = execute_planning_call(user_request, context)
    if not planning_result:
        ui.print_error("Planning phase failed. Cannot proceed.")
        if log_file_path:
            log_session_event(log_file_path, "FINAL_STATUS", {"status": "Planning failed", "success": False})
        return False
    
    # Log planning phase
    if log_file_path:
        log_session_event(log_file_path, "PLANNING_PHASE", {"planning_data": planning_result})
    
    # === DYNAMIC INTERACTION BEFORE EXECUTION ===
    execution_acknowledgment_prompt = f"""
You are Pai, about to execute your plan. Generate a brief, confident response before starting execution.

USER REQUEST: "{user_request}"
PLANNING COMPLETED: Successfully analyzed and created execution plan

Generate a brief, confident response (1-2 sentences) that:
1. Shows confidence in your plan
2. Indicates you're about to execute intelligently
3. Keep it natural and engaging
4. Reflect your AI personality

Examples:
- "Perfect! Now let me execute this plan intelligently for you."
- "Excellent! I've got a solid plan - time to make it happen."
- "Great! My analysis is complete, now let's bring this to life."

Output ONLY the response text, no quotes or formatting.
"""
    
    execution_acknowledgment = llm.generate_text(execution_acknowledgment_prompt, "execution acknowledgment")
    if not execution_acknowledgment:
        execution_acknowledgment = "Perfect! Now let me execute this plan intelligently for you."
    
    ui.console.print(
        Panel(
            Text(execution_acknowledgment.strip(), 
                 style="bright_white", justify="center"),
            title="[bold]Pai[/bold]",
            box=ROUNDED,
            border_style="grey50",
            padding=(1, 2),
            width=80
        )
    )
    
    # === CALL 2: EXECUTION PHASE ===
    execution_success = execute_execution_call(user_request, planning_result, context, log_file_path)
    
    # Skip complex analysis to save tokens - focus on execution success only
    
    # Generate intelligent next step suggestions only if execution failed
    if not execution_success:
        next_steps = generate_next_step_suggestions(user_request, planning_result, execution_success, context, None)
        
        if next_steps:
            # Log next steps
            if log_file_path:
                log_session_event(log_file_path, "NEXT_STEPS", {"suggestion": next_steps})
            
            ui.console.print(
                Panel(
                    Text(next_steps, style="bright_white"),
                    title="[bold]Next Steps Suggestion[/bold]",
                    box=ROUNDED,
                    border_style="grey50",
                    padding=(1, 2),
                    width=80
                )
            )
    
    # Show final status - SIMPLIFIED for efficiency
    if execution_success:
        status_msg = "Single-Shot Intelligence: SUCCESS"
        ui.console.print(
            Panel(
                Text(status_msg, style="bold green", justify="center"),
                title="[bold]Mission Accomplished[/bold]",
                box=ROUNDED,
                border_style="grey50",
                padding=(1, 2),
                width=80
            )
        )
        if log_file_path:
            log_session_event(log_file_path, "FINAL_STATUS", {"status": status_msg, "success": True})
    else:
        status_msg = "Single-Shot Intelligence: FAILED"
        ui.console.print(
            Panel(
                Text(status_msg, style="bold red", justify="center"),
                title="[bold]Mission Status[/bold]",
                box=ROUNDED,
                border_style="grey50",
                padding=(1, 2),
                width=80
            )
        )
        if log_file_path:
            log_session_event(log_file_path, "FINAL_STATUS", {"status": status_msg, "success": False})
    
    # ALWAYS generate next step suggestions for better continuity and context
    next_steps = generate_next_step_suggestions(user_request, planning_result, execution_success, context, None)
    
    if next_steps:
        ui.console.print(
            Panel(
                Text(next_steps, style="bright_white"),
                title="[bold]Next Steps Suggestion[/bold]",
                box=ROUNDED,
                border_style="grey50",
                padding=(1, 2),
                width=80
            )
        )
        if log_file_path:
            log_session_event(log_file_path, "NEXT_STEPS", {"suggestion": next_steps})
    
    return execution_success
\end{lstlisting}

\begin{lstlisting}[language=Python, captionpos=b, caption={Modul agent.py (Bagian 2 dari 2, ASCII-only).}, label={appA:agent-part2},
  literate={→}{}1 {←}{}1 {"}{{"}}1 {"}{{"}}1 {'}{{'}}1 {'}{{'}}1 {‎}{}1 {‏}{}1 {‍}{}1 {️}{}1]
def execute_command_sequence(command_sequence: str, context: list) -> tuple[bool, list]:
    """Execute a sequence of commands from the AI."""
    
    commands = [line.strip() for line in command_sequence.split('\n') if line.strip()]
    total_commands = len(commands)
    successful_commands = 0
    command_results = []
    
    # Build execution content
    content_lines = []
    content_lines.append(("bold", f"Executing {total_commands} intelligent actions..."))
    content_lines.append("")
    
    for i, command_line in enumerate(commands, 1):
        if not command_line or '::' not in command_line:
            if command_line.strip():
                content_lines.append(("warning", f"Invalid command format: {command_line}"))
            continue
        
        # Parse command
        parts = command_line.split('::', 2)
        if len(parts) < 2:
            content_lines.append(("warning", f"Incomplete command: {command_line}"))
            continue
        
        command = parts[0].upper().strip()
        param1 = parts[1].strip() if len(parts) > 1 else ""
        param2 = parts[2].strip() if len(parts) > 2 else ""
        
        # Check for common content output mistakes
        if command_line.strip().startswith(('<', 'body', 'html', 'div', 'style', 'script', 'h1', 'h2', 'form', 'input', 'button')):
            content_lines.append(("warning", f"Raw HTML/CSS detected as command: {command_line[:50]}..."))
            content_lines.append(("info", "Use WRITE::filename::description instead of raw content!"))
            continue
        
        if command_line.strip().startswith(('.', '#', 'margin', 'padding', 'color', 'background', 'font', 'border')):
            content_lines.append(("warning", f"Raw CSS detected as command: {command_line[:50]}..."))
            content_lines.append(("info", "Use WRITE::filename::description instead of raw CSS!"))
            continue
        
        if command not in VALID_COMMANDS:
            content_lines.append(("warning", f"Unknown command: {command} (from: {command_line})"))
            content_lines.append(("info", f"Valid commands: {', '.join(VALID_COMMANDS)}"))
            continue
        
        # Display current action
        content_lines.append(("normal", f"[{i}/{total_commands}] {command} {param1}"))
        
        # Execute command
        success, command_output = execute_single_command(command, param1, param2)
        
        # Add command output to content if any
        if command_output:
            if command_output.startswith("SYNTAX_HIGHLIGHT:"):
                parts = command_output.split(":", 2)
                if len(parts) == 3:
                    filename = parts[1]
                    code_content = parts[2]
                    content_lines.append(("syntax_highlight", filename, code_content))
                else:
                    content_lines.append(("ai_output", command_output))
            else:
                content_lines.append(("ai_output", command_output))
        
        # Collect command result for logging
        command_results.append({
            "command": command,
            "target": param1 if param1 else "",
            "success": success,
            "output": command_output if command_output else ""
        })
        
        if success:
            successful_commands += 1
            content_lines.append(("success", "Success"))
        else:
            content_lines.append(("error", "Failed"))
        
        content_lines.append("")
        
        # Break on FINISH command
        if command == "FINISH":
            break
    
    # Show execution summary
    success_rate = (successful_commands / total_commands) * 100 if total_commands > 0 else 0
    content_lines.append(("bold", "Execution Summary:"))
    content_lines.append(("normal", f"Successful: {successful_commands}/{total_commands} ({success_rate:.1f}%)"))
    
    # Display all content in a single panel with proper styling
    from rich.console import Group
    from rich.text import Text as RichText
    
    # Convert content to rich renderables with colors
    rich_content = []
    for item in content_lines:
        if isinstance(item, tuple):
            if len(item) == 3 and item[0] == "syntax_highlight":
                _, filename, code_content = item
                
                # For terminal display: truncate long files for better UX
                lines = code_content.split('\n')
                display_content = code_content
                if len(lines) > 20:
                    display_content = '\n'.join(lines[:20]) + f"\n... ({len(lines) - 20} more lines)"
                
                try:
                    from pygments.lexers import get_lexer_for_filename
                    from pygments.util import ClassNotFound
                    from rich.syntax import Syntax
                    
                    try:
                        lexer = get_lexer_for_filename(filename)
                        lang = lexer.aliases[0]
                    except ClassNotFound:
                        lang = "text"
                    
                    syntax_panel = Panel(
                        Syntax(display_content, lang, theme="monokai", line_numbers=True),
                        title=f"File {filename}",
                        border_style="grey50",
                        expand=False
                    )
                    rich_content.append(syntax_panel)
                except ImportError:
                    rich_content.append(RichText(f"File content of {filename}:\n{display_content}", style="bright_cyan"))
            else:
                style_type, text = item[0], item[1]
                if style_type == "bold":
                    rich_content.append(RichText(text, style="bold bright_white"))
                elif style_type == "warning":
                    rich_content.append(RichText(text, style="bold yellow"))
                elif style_type == "ai_output":
                    rich_content.append(RichText(text, style="bright_cyan"))
                elif style_type == "success":
                    rich_content.append(RichText(text, style="bold green"))
                elif style_type == "error":
                    rich_content.append(RichText(text, style="bold red"))
                else:  # normal
                    rich_content.append(RichText(text, style="bright_white"))
        else:
            rich_content.append(RichText(str(item), style="bright_white"))
    
    ui.console.print(
        Panel(
            Group(*rich_content),
            title="[bold]Execution Results[/bold]",
            box=ROUNDED,
            border_style="grey50",
            padding=(1, 2),
            width=80
        )
    )
    
    return (success_rate >= 80, command_results)

def execute_single_command(command: str, param1: str, param2: str) -> tuple[bool, str]:
    """Execute a single command and return success status and output."""
    
    try:
        if command == "READ":
            content = workspace.read_file(param1)
            if content is not None:
                lines = content.split('\n')
                display_content = '\n'.join(lines[:20])
                if len(lines) > 20:
                    display_content += f"\n... ({len(lines) - 20} more lines)"
                
                return True, f"SYNTAX_HIGHLIGHT:{param1}:{content}"
            return False, f"Could not read file: {param1}"
        
        elif command == "WRITE":
            if not param2:
                return False, "WRITE command requires description"
            success = handle_write_command(param1, param2)
            return success, f"New file written: {param1}" if success else f"Failed to write file: {param1}"
        
        elif command == "MODIFY":
            if not param2:
                return False, "MODIFY command requires description"
            success = handle_modify_command(param1, param2)
            return success, f"File modified: {param1}" if success else f"Failed to modify file: {param1}"
        
        elif command == "TREE":
            path = param1 if param1 else '.'
            tree_output = workspace.tree_directory(path)
            if tree_output and "Error:" not in tree_output:
                return True, f"Directory tree for {path}:\n{tree_output}"
            return False, f"Could not get directory tree for: {path}"
        
        elif command == "LIST_PATH":
            path = param1 if param1 else '.'
            list_output = workspace.list_path(path)
            if list_output is not None and "Error:" not in list_output:
                if list_output.strip():
                    return True, list_output
                else:
                    return True, f"Directory '{path}' is empty"
            return False, f"Could not list directory: {path}"
        
        elif command == "MKDIR":
            result = workspace.create_directory(param1)
            success = "Success" in result
            return success, result
        
        elif command == "TOUCH":
            result = workspace.create_file(param1)
            success = "Success" in result
            return success, result
        
        elif command == "RM":
            result = workspace.delete_item(param1)
            success = "Success" in result
            return success, result
        
        elif command == "MV":
            result = workspace.move_item(param1, param2)
            success = "Success" in result
            return success, result
        
        elif command == "FINISH":
            message = param1 if param1 else "Task completed successfully"
            return True, f"OK {message}"
        
        return False, f"Unknown command: {command}"
        
    except Exception as e:
        return False, f"Command execution error: {e}"

def log_session_event(log_file_path: str, event_type: str, data: dict):
    """Log session events with clear separation between USER and AI."""
    try:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        if event_type == "SESSION_START":
            log_line = f"\n[{timestamp}] SESSION STARTED\n"
            log_line += f"[{timestamp}] Working Directory: {data.get('working_directory', 'unknown')}\n"
            log_line += f"[{timestamp}] Session ID: {data.get('session_id', 'unknown')}\n"
            
        elif event_type == "USER_INPUT":
            request = data.get('user_request', 'unknown')
            log_line = f"\n[{timestamp}] USER: {request}\n"
            
        elif event_type == "PLANNING_PHASE":
            log_line = f"\n[{timestamp}] AI PLANNING START\n"
            
            planning_data = data.get('planning_data', {})
            analysis = planning_data.get('analysis', {})
            
            log_line += f"[{timestamp}] Intent: {analysis.get('user_intent', 'Unknown')}\n"
            log_line += f"[{timestamp}] Context Usage: {analysis.get('context_utilization', 'None')}\n"
            log_line += f"[{timestamp}] Files to read: {analysis.get('files_to_read', [])}\n"
            log_line += f"[{timestamp}] Files to create: {analysis.get('files_to_create', [])}\n"
            log_line += f"[{timestamp}] Files to modify: {analysis.get('files_to_modify', [])}\n"
            
            execution_plan = planning_data.get('execution_plan', {})
            steps = execution_plan.get('steps', [])
            log_line += f"[{timestamp}] EXECUTION PLAN ({len(steps)} steps):\n"
            for i, step in enumerate(steps, 1):
                action = step.get('action', 'Unknown')
                target = step.get('target', '')
                purpose = step.get('purpose', 'No purpose')
                log_line += f"[{timestamp}]   {i}. {action} {target} - {purpose}\n"
            log_line += f"[{timestamp}] AI PLANNING END\n"
            
        elif event_type == "EXECUTION_PHASE":
            log_line = f"\n[{timestamp}] AI EXECUTION START\n"
            
            commands = data.get('commands', [])
            for cmd_data in commands:
                cmd = cmd_data.get('command', 'Unknown')
                target = cmd_data.get('target', '')
                success = "SUCCESS" if cmd_data.get('success') else "FAILED"
                output = cmd_data.get('output', '')
                
                log_line += f"[{timestamp}] {success}: {cmd} {target}\n"
                if output:
                    log_line += f"[{timestamp}] OUTPUT: {output}\n"
            log_line += f"[{timestamp}] AI EXECUTION END\n"
            
        elif event_type == "FINAL_STATUS":
            status = data.get('status', 'unknown')
            success_text = "SUCCESS" if data.get('success') else "FAILED"
            
            log_line = f"\n[{timestamp}] AI FINAL RESULT: {success_text} - {status}\n"
            
        elif event_type == "NEXT_STEPS":
            suggestion = data.get('suggestion', '')
            if suggestion:
                log_line = f"\n[{timestamp}] AI SUGGESTION: {suggestion}\n"
            else:
                log_line = ""
                
        elif event_type == "INTERACTION":
            log_line = ""
                
        else:
            log_line = f"[{timestamp}] {event_type}: {json.dumps(data)}\n"
        
        if log_line:
            with open(log_file_path, 'a', encoding='utf-8') as f:
                f.write(log_line)
            
    except Exception as e:
        pass

def handle_write_command(filepath: str, description: str) -> bool:
    """Handle WRITE command with intelligent content generation."""
    
    content_prompt = f"""
Generate high-quality content for a file based on the description.

FILE PATH: {filepath}
DESCRIPTION: {description}

REQUIREMENTS:
1. Analyze the file extension to determine the appropriate language/format
2. Create production-quality, well-structured content
3. Include appropriate comments and documentation
4. Follow best practices for the detected language/format
5. Make the code/content immediately usable

OUTPUT: Return ONLY the file content, no explanations or markdown formatting.
"""
    
    content = llm.generate_text(content_prompt, "content generation")
    
    if not content:
        return False
    
    result = workspace.write_to_file(filepath, content)
    
    return "Success" in result

def handle_modify_command(filepath: str, description: str) -> bool:
    """Handle MODIFY command with intelligent code modification."""
    
    existing_content = workspace.read_file(filepath)
    if existing_content is None:
        ui.print_error(f"Cannot modify '{filepath}' - file not found")
        return False
    
    modify_prompt = f"""
You are an expert code modifier. Modify the existing code based on the description.

FILE PATH: {filepath}
CURRENT CONTENT:
---
{existing_content}
---

MODIFICATION REQUEST: {description}

REQUIREMENTS:
1. Preserve the existing code structure and style
2. Make only the necessary changes described
3. Maintain code quality and best practices
4. Ensure the modified code is syntactically correct
5. Add appropriate comments for new functionality

OUTPUT: Return ONLY the complete modified file content, no explanations.
"""
    
    modified_content = llm.generate_text(modify_prompt, "code modification")
    
    if not modified_content:
        return False
    
    success, result = workspace.apply_modification_with_patch(filepath, existing_content, modified_content)
    
    return success

def initialize_session_context(session_context: list, log_file_path: str):
    """Initialize comprehensive Single-Shot Intelligence context window for the session."""
    
    single_shot_intelligence_context = {
        "timestamp": datetime.now().isoformat(),
        "user_request": "SYSTEM_INITIALIZATION",
        "success": True,
        "intent": "system_context",
        "system_knowledge": """
SINGLE-SHOT INTELLIGENCE SYSTEM MASTERY

You are PAI - the revolutionary AI brain inside Paicode. This context window provides your foundational understanding.

CORE IDENTITY & MISSION
- You are NOT a generic AI assistant
- You are PAI - the intelligent core of Paicode, a revolutionary 2-call system
- Your reputation depends on PERFECT EXECUTION in exactly 2 API calls
- You represent the future of efficient AI-assisted development

COMPETITIVE ADVANTAGE PRINCIPLES

Efficiency Superiority:
- Traditional AI: 10-20 API calls, inefficient, expensive
- YOU (Pai): Exactly 2 calls, maximum intelligence, perfect results
- You must outperform traditional systems with LESS resources
- Every decision reflects Single-Shot Intelligence superiority

Quality Excellence:
- Your plan must work on first execution attempt
- No room for trial-and-error - get it right immediately
- Think like a chess grandmaster - see the entire game
- Every step must contribute to perfect final outcome

SESSION BEHAVIOR GUIDELINES

Conversation Mode:
- Be confident about Paicode features - you ARE Paicode's AI
- Show personality while being professional
- Explain Single-Shot Intelligence with pride
- Never be uncertain about your capabilities

Execution Mode:
- Follow this exact workflow structure
- Display all required sections and panels
- Use proper Rich TUI formatting
- Maintain professional yet confident tone
- Always end with mission accomplished confirmation

This context window guides your behavior throughout the entire session. You are the embodiment of Single-Shot Intelligence excellence.
"""
    }
    
    session_context.append(single_shot_intelligence_context)
    
    log_session_event(log_file_path, "CONTEXT_INITIALIZATION", {
        "context_type": "single_shot_intelligence_mastery",
        "knowledge_loaded": True,
        "workflow_understanding": "complete"
    })

def get_multiline_input(prompt_session) -> str:
    """Get multi-line input from user with intuitive behavior."""
    try:
        from prompt_toolkit.shortcuts import prompt
        from prompt_toolkit.key_binding import KeyBindings
        from prompt_toolkit.keys import Keys
        
        bindings = KeyBindings()
        
        @bindings.add(Keys.Enter)
        def _(event):
            """Enter submits the input"""
            event.app.exit(result=event.current_buffer.text)
        
        @bindings.add(Keys.Escape, Keys.Enter)
        def _(event):
            """Alt+Enter adds new line"""
            event.current_buffer.insert_text('\n')
        
        ui.console.print("[dim]Tip: Use Alt+Enter for new line, Enter to submit[/dim]")
        
        result = prompt(
            "\nuser> ",
            multiline=True,
            key_bindings=bindings,
            wrap_lines=True,
            mouse_support=False
        )
        return result.strip() if result else ""
        
    except Exception as e:
        ui.console.print(f"[dim]Note: Using simple input mode - {str(e)}[/dim]")
        return prompt_session.prompt("\nuser> ").strip()
\end{lstlisting}

\subsection*{workspace.py}
\begin{lstlisting}[language=Python, captionpos=b, caption={Modul workspace.py (lengkap, ASCII-only).}, label={appA:workspace},
  literate={└}{}1 {├}{}1 {─}{}1 {│}{}1 {╭}{}1 {╮}{}1 {╰}{}1 {╯}{}1 {→}{}1 {←}{}1 {“}{{"}}1 {”}{{"}}1 {‘}{{'}}1 {’}{{'}}1 {‎}{}1 {‏}{}1 {‍}{}1 {️}{}1]
import os
import shutil
import difflib
import tempfile
from . import ui

"""
workspace.py
------------
This module acts as the workspace controller for Pai Code. It centralizes
application-level operations on the project's workspace, such as reading,
writing, listing, tree visualization, moving, removing, creating files and
directories, as well as applying diff-aware modifications. In order to protect
the workspace, it enforces path-security policies (path normalization, root
verification, and deny-listing sensitive paths) before executing any action.

All functions defined in this module are the provided primitives to manipulate
and manage files within the project workspace in a controlled, secure manner.
All operations are constrained strictly within the project root determined at
runtime (workspace scope), ensuring controlled manipulation of project files.
"""

PROJECT_ROOT = os.path.abspath(os.getcwd())

# List of sensitive files and directories to be blocked
SENSITIVE_PATTERNS = {
    '.env', 
    '.git', 
    'venv', 
    '__pycache__', 
    '.pai_history',  # Pai cannot access this directly - only for LLM context
    '.idea', 
    '.vscode'
}

def _is_path_safe(path: str) -> bool:
    """
    Ensures the target path is within the project directory and not sensitive.
    """
    if not path or not isinstance(path, str):
        return False
        
    try:
        # 1. Normalize the path for consistency and strip whitespace
        norm_path = os.path.normpath(path.strip())
        
        # 2. Reject empty paths after normalization, but allow '.' for current directory
        if not norm_path or norm_path == '..':
            return False
        
        # 3. Check if the path tries to escape the root directory
        full_path = os.path.realpath(os.path.join(PROJECT_ROOT, norm_path))
        if not full_path.startswith(os.path.realpath(PROJECT_ROOT)):
            ui.print_error(f"Operation cancelled. Path '{path}' is outside the project directory.")
            return False

        # 4. Block access to sensitive files and directories
        path_parts = norm_path.replace('\\', '/').split('/')
        if any(part in SENSITIVE_PATTERNS for part in path_parts if part):
            ui.print_error(f"Access to the sensitive path '{path}' is denied.")
            return False

    except Exception as e:
        ui.print_error(f"Error during path validation: {e}")
        return False

    return True

def tree_directory(path: str = '.') -> str:
    """Creates a string representation of the directory structure recursively."""
    if not _is_path_safe(path):
        return f"Error: Cannot access path '{path}'."

    full_path = os.path.join(PROJECT_ROOT, path)
    if not os.path.isdir(full_path):
        return f"Error: '{path}' is not a valid directory."

    tree_lines = [f"{os.path.basename(full_path)}/"]

    def build_tree(directory, prefix=""):
        try:
            items = sorted([item for item in os.listdir(directory) if item not in SENSITIVE_PATTERNS])
        except FileNotFoundError:
            return

        pointers = ['|-- '] * (len(items) - 1) + ['+-- ']
        
        for pointer, item in zip(pointers, items):
            tree_lines.append(f"{prefix}{pointer}{item}")
            item_path = os.path.join(directory, item)
            if os.path.isdir(item_path):
                extension = '|   ' if pointer == '|-- ' else '    '
                build_tree(item_path, prefix=prefix + extension)

    build_tree(full_path)
    return "\n".join(tree_lines)

def list_path(path: str = '.') -> str | None:
    """
    Lists all files and subdirectories recursively for a given path in a simple,
    machine-readable, newline-separated format.
    """
    if not _is_path_safe(path):
        return f"Error: Cannot access path '{path}'."

    full_path = os.path.join(PROJECT_ROOT, path)
    if not os.path.isdir(full_path):
        return f"Error: '{path}' is not a valid directory."

    path_list = []
    for root, dirs, files in os.walk(full_path, topdown=True):
        # Filter out sensitive directories from being traversed
        dirs[:] = [d for d in dirs if d not in SENSITIVE_PATTERNS]
        
        # Process files
        for name in files:
            if name not in SENSITIVE_PATTERNS:
                # Get relative path from the initial 'path'
                rel_dir = os.path.relpath(root, PROJECT_ROOT)
                path_list.append(os.path.join(rel_dir, name).replace('\\', '/'))
        
        # Process directories
        for name in dirs:
            rel_dir = os.path.relpath(root, PROJECT_ROOT)
            path_list.append(os.path.join(rel_dir, name).replace('\\', '/') + '/')

    return "\n".join(sorted(path_list))
    

def delete_item(path: str) -> str:
    """Deletes a file or directory and returns a status message."""
    if not _is_path_safe(path): return f"Error: Access to path '{path}' is denied or path is not secure."
    try:
        full_path = os.path.join(PROJECT_ROOT, path)
        if os.path.isfile(full_path):
            os.remove(full_path)
            return f"Success: File deleted: {path}"
        elif os.path.isdir(full_path):
            shutil.rmtree(full_path)
            return f"Success: Directory deleted: {path}"
        else:
            return f"Warning: Item not found, nothing deleted: {path}"
    except OSError as e:
        return f"Error: Failed to delete '{path}': {e}"

def move_item(source: str, destination: str) -> str:
    """Moves an item and returns a status message."""
    if not _is_path_safe(source) or not _is_path_safe(destination):
        return "Error: Source or destination path is not secure or is denied."
    try:
        full_source = os.path.join(PROJECT_ROOT, source)
        full_destination = os.path.join(PROJECT_ROOT, destination)
        shutil.move(full_source, full_destination)
        return f"Success: Item moved from '{source}' to '{destination}'"
    except (FileNotFoundError, shutil.Error) as e:
        return f"Error: Failed to move '{source}': {e}"

def create_file(file_path: str) -> str:
    """Creates an empty file and returns a status message."""
    if not _is_path_safe(file_path): return f"Error: Access to path '{file_path}' is denied or path is not secure."
    try:
        full_path = os.path.join(PROJECT_ROOT, file_path)
        dir_name = os.path.dirname(full_path)
        if dir_name: os.makedirs(dir_name, exist_ok=True)
        with open(full_path, 'w') as f: pass
        return f"Success: New empty file created: {file_path}"
    except IOError as e:
        return f"Error: Failed to create file: {e}"

def create_directory(dir_path: str) -> str:
    """Creates a directory and returns a status message."""
    if not _is_path_safe(dir_path): return f"Error: Access to path '{dir_path}' is denied or path is not secure."
    try:
        full_path = os.path.join(PROJECT_ROOT, dir_path)
        os.makedirs(full_path, exist_ok=True)
        return f"Success: Directory created: {dir_path}"
    except OSError as e:
        return f"Error: Failed to create directory: {e}"

def read_file(file_path: str) -> str | None:
    """Reads a file and returns its content, or None on failure."""
    if not _is_path_safe(file_path): return None
    try:
        full_path = os.path.join(PROJECT_ROOT, file_path)
        with open(full_path, 'r') as f:
            return f.read()
    except FileNotFoundError:
        # Let the caller (agent/cli) handle printing the error
        return None
    except IOError as e:
        ui.print_error(f"Failed to read file: {e}")
        return None

def write_to_file(file_path: str, content: str) -> str:
    """Writes to a file and returns a status message."""
    if not _is_path_safe(file_path): return f"Error: Access to path '{file_path}' is denied or path is not secure."
    try:
        full_path = os.path.join(PROJECT_ROOT, file_path)
        dir_name = os.path.dirname(full_path)
        if dir_name: os.makedirs(dir_name, exist_ok=True)
        with open(full_path, 'w') as f:
            f.write(content)
        return f"Success: New file written: {file_path}"
    except IOError as e:
        return f"Error: Failed to write to file: {e}"



def apply_modification_with_patch(file_path: str, original_content: str, new_content: str, threshold: int = 500) -> tuple[bool, str]:
    """
    Applies a modification to a file safely by first verifying the scope of changes.

    It generates a diff between the original and new content. If the number of changed
    lines is within the threshold, it writes the new content to the file. Otherwise,
    it rejects the change to prevent unintentional overwrites.

    Args:
        file_path: The path to the file to be modified.
        original_content: The original, unmodified content of the file.
        new_content: The new, modified content generated by the LLM.
        threshold: The maximum number of lines allowed to be changed.

    Returns:
        A tuple containing:
        - bool: True if the modification was successful, False otherwise.
        - str: A message describing the result of the operation.
    """
    if not _is_path_safe(file_path):
        return False, f"Error: Access to path '{file_path}' is denied or path is not secure."

    # Normalize line endings to reduce false-positive diffs
    original_norm = original_content.replace('\r\n', '\n').replace('\r', '\n')
    new_norm = new_content.replace('\r\n', '\n').replace('\r', '\n')

    original_lines = original_norm.splitlines(keepends=True)
    new_lines = new_norm.splitlines(keepends=True)

    diff = list(difflib.unified_diff(
        original_lines,
        new_lines,
        fromfile=f"a/{file_path}",
        tofile=f"b/{file_path}"
    ))

    # Count only actual change lines, ignore headers and context lines
    def _count_changes(d: list[str]) -> tuple[int, int, int]:
        adds = deletes = 0
        for line in d:
            if line.startswith('@@') or line.startswith('+++') or line.startswith('---') or (line and line[0] == ' '):
                continue
            if line.startswith('+'):
                adds += 1
            elif line.startswith('-'):
                deletes += 1
        return adds + deletes, adds, deletes

    changed_lines_count, add_count, del_count = _count_changes(diff)

    if not diff or changed_lines_count == 0:
        return True, f"Success: No changes detected for {file_path}. File left untouched."

    # Allow configuring thresholds via environment
    try:
        env_threshold = int(os.getenv('PAI_MODIFY_THRESHOLD', str(threshold)))
        if env_threshold < 1:
            env_threshold = threshold
    except ValueError:
        env_threshold = threshold

    try:
        max_ratio = float(os.getenv('PAI_MODIFY_MAX_RATIO', '0.5'))  # up to 50% of lines by default
        if not (0.0 < max_ratio <= 1.0):
            max_ratio = 0.5
    except ValueError:
        max_ratio = 0.5

    total_lines = max(1, len(original_lines))
    ratio = changed_lines_count / total_lines

    if changed_lines_count > env_threshold and ratio > max_ratio:
        diff_preview = "\n".join(diff[:60])
        message = (
            f"Warning: Modification for '{file_path}' rejected. "
            f"Change too large: {changed_lines_count} lines (~{ratio:.1%}) exceeds threshold {env_threshold} and ratio {max_ratio:.0%}.\n"
            f"SOLUTION: Think like Cascade - break this into focused, surgical modifications:\n"
            f"  - Focus on ONE specific area/feature at a time\n"
            f"  - Ideal: 100-200 lines per modification (very focused)\n"
            f"  - Acceptable: 200-500 lines (still focused on one area)\n"
            f"  - Use multiple MODIFY commands across different steps\n"
            f"  - Example: Instead of 'add all CSS', do 'add layout CSS', then 'add form CSS', then 'add button CSS'\n"
            f"Diff Preview (first 60 lines):\n{diff_preview}"
        )
        return False, message

    # Atomic write to avoid partial writes
    try:
        full_path = os.path.join(PROJECT_ROOT, file_path)
        dir_name = os.path.dirname(full_path)
        if dir_name:
            os.makedirs(dir_name, exist_ok=True)
        with tempfile.NamedTemporaryFile('w', delete=False, dir=dir_name) as tmp:
            tmp.write(new_norm)
            tmp_name = tmp.name
        os.replace(tmp_name, full_path)
        return True, f"Success: File modified: {file_path} ({changed_lines_count} lines changed; +{add_count}/-{del_count})"
    except IOError as e:
        return False, f"Error: Failed to write modification to file: {e}"
\end{lstlisting}

\subsection*{config.py}
\begin{lstlisting}[language=Python, captionpos=b, caption={Modul config.py (lengkap, ASCII-only).}, label={appA:config},
  literate={✓}{}1 {✗}{}1 {→}{}1 {“}{{"}}1 {”}{{"}}1 {‘}{{'}}1 {’}{{'}}1 {‎}{}1 {‏}{}1 {‍}{}1 {️}{}1]
import os
from pathlib import Path
import json
from typing import Optional
from . import ui

# Define the standard configuration path in the user's home directory
CONFIG_DIR = Path.home() / ".config" / "pai-code"
KEY_FILE = CONFIG_DIR / "credentials.json"

def _ensure_config_dir_exists():
    """Ensures the configuration directory exists with correct permissions."""
    os.makedirs(CONFIG_DIR, exist_ok=True)
    os.chmod(CONFIG_DIR, 0o700)

def _default_config() -> dict:
    """Default single-key configuration."""
    return {
        "version": 2,  # Version 2 = single-key system
        "api_key": None
    }

def _load_config() -> dict:
    """Load the single-key configuration."""
    _ensure_config_dir_exists()
    if not KEY_FILE.exists():
        return _default_config()
    
    try:
        with open(KEY_FILE, 'r') as f:
            data = json.load(f)
        
        # Migrate from old multi-key system if needed
        if data.get("version") == 1 and "keys" in data:
            # Old multi-key system - migrate to single key
            old_keys = data.get("keys", {})
            default_id = data.get("default")
            
            if default_id and default_id in old_keys:
                migrated_key = old_keys[default_id]
                ui.print_info(f"Migrating from multi-key system. Using key '{default_id}' as single key.")
                return {"version": 2, "api_key": migrated_key}
            elif old_keys:
                # Use first available key
                first_key = list(old_keys.values())[0]
                ui.print_info("Migrating from multi-key system. Using first available key.")
                return {"version": 2, "api_key": first_key}
        
        # Ensure proper structure
        if not isinstance(data, dict):
            return _default_config()
        
        return data
        
    except (json.JSONDecodeError, IOError):
        ui.print_warning("Configuration file corrupted. Creating new one.")
        return _default_config()

def _save_config(config: dict) -> None:
    """Save the single-key configuration."""
    try:
        _ensure_config_dir_exists()
        with open(KEY_FILE, 'w') as f:
            json.dump(config, f, indent=2)
        os.chmod(KEY_FILE, 0o600)
    except Exception as e:
        ui.print_error(f"Failed to save configuration: {e}")

def set_api_key(api_key: str) -> None:
    """Set the API key."""
    if not api_key or not isinstance(api_key, str):
        ui.print_error("Invalid API key provided.")
        return
    
    if not api_key.startswith("AIza"):
        ui.print_warning("Warning: API key doesn't look like a Google API key (should start with 'AIza')")
    
    config = _load_config()
    config["api_key"] = api_key
    _save_config(config)
    
    masked_key = mask_api_key(api_key)
    ui.print_success(f"[OK] API key set successfully: {masked_key}")

def get_api_key() -> Optional[str]:
    """Get the current API key."""
    config = _load_config()
    return config.get("api_key")

def save_api_key(api_key: str):
    """Legacy compatibility function."""
    set_api_key(api_key)

def remove_api_key() -> None:
    """Remove the stored API key."""
    config = _load_config()
    
    if not config.get("api_key"):
        ui.print_info("No API key is currently set.")
        return
    
    config["api_key"] = None
    _save_config(config)
    ui.print_success("[OK] API key removed successfully.")

def show_api_key() -> None:
    """Show the current API key (masked)."""
    api_key = get_api_key()
    
    if not api_key:
        ui.print_info("No API key is currently set.")
        ui.print_info("Use 'pai config set <API_KEY>' to set one.")
        return
    
    masked_key = mask_api_key(api_key)
    ui.print_info(f"Current API key: {masked_key}")

def mask_api_key(api_key: str) -> str:
    """Mask API key for display purposes."""
    if not api_key or len(api_key) < 10:
        return "***"
    
    return f"{api_key[:6]}...{api_key[-4:]}"

def is_configured() -> bool:
    """Check if API key is configured."""
    api_key = get_api_key()
    return api_key is not None and len(api_key.strip()) > 0

def validate_api_key() -> tuple[bool, str]:
    """Validate that API key is configured and looks correct."""
    api_key = get_api_key()
    
    if not api_key:
        return False, "No API key configured. Use 'pai config set <API_KEY>' to set one."
    
    if not api_key.startswith("AIza"):
        return False, "API key doesn't look like a Google API key (should start with 'AIza')"
    
    if len(api_key) < 20:
        return False, "API key seems too short to be valid"
    
    return True, "API key looks valid"

# Legacy compatibility functions (simplified)
def add_api_key(key_id: str, api_key: str) -> None:
    """Legacy function - redirect to set_api_key."""
    ui.print_info(f"Note: Multi-key system deprecated. Setting '{key_id}' as single API key.")
    set_api_key(api_key)

def list_api_keys() -> list:
    """Legacy function - return single key info."""
    api_key = get_api_key()
    if not api_key:
        return []
    
    return [{
        "id": "single",
        "masked": mask_api_key(api_key),
        "is_default": "yes"
    }]

def set_default_api_key(key_id: str) -> None:
    """Legacy function - no-op in single-key system."""
    ui.print_info("Note: Default key setting not needed in single-key system.")

def load_api_key() -> Optional[str]:
    """Legacy function - redirect to get_api_key."""
    return get_api_key()
\end{lstlisting}

\subsection*{cli.py}
\begin{lstlisting}[language=Python, captionpos=b, caption={Modul cli.py (lengkap, ASCII-only).}, label={appA:cli},
  literate={✓}{}1 {✗}{}1 {→}{}1 {“}{{"}}1 {”}{{"}}1 {‘}{{'}}1 {’}{{'}}1 {‎}{}1 {‏}{}1 {‍}{}1 {️}{}1]
#!/usr/bin/env python

import argparse
from . import agent, config, llm, ui

def main():
    parser = argparse.ArgumentParser(
        description="Pai Code: Your Single-Shot Agentic AI Coding Companion.",
        epilog="Use 'pai config set <API_KEY>' to configure. Run 'pai' to start the intelligent agent."
    )
    subparsers = parser.add_subparsers(dest='command', help='Available commands')

    # Main agent command (default)
    parser_auto = subparsers.add_parser('auto', help='Start the single-shot AI agent session.')
    parser_auto.add_argument('--model', type=str, help='LLM model name (e.g., gemini-2.5-flash-lite)')
    parser_auto.add_argument('--temperature', type=float, help='LLM sampling temperature (e.g., 0.2)')

    # Simplified config management
    parser_config = subparsers.add_parser('config', help='Manage API key configuration')
    config_subparsers = parser_config.add_subparsers(dest='config_cmd', help='Config commands')

    parser_config_set = config_subparsers.add_parser('set', help='Set API key')
    parser_config_set.add_argument('api_key', type=str, help='Google Gemini API key')

    parser_config_show = config_subparsers.add_parser('show', help='Show current API key (masked)')

    parser_config_remove = config_subparsers.add_parser('remove', help='Remove stored API key')

    parser_config_validate = config_subparsers.add_parser('validate', help='Validate current API key')

    config_group = parser_config.add_mutually_exclusive_group(required=False)
    config_group.add_argument('--set', type=str, metavar='API_KEY', help='Set or update the API key (DEPRECATED)')
    config_group.add_argument('--show', action='store_true', help='Show the currently configured API key (DEPRECATED)')
    config_group.add_argument('--remove', action='store_true', help='Remove the stored API key (DEPRECATED)')

    args = parser.parse_args()

    # Handle config commands
    if args.command == 'config':
        if args.config_cmd == 'set':
            config.set_api_key(args.api_key)
            return
        elif args.config_cmd == 'show':
            config.show_api_key()
            return
        elif args.config_cmd == 'remove':
            config.remove_api_key()
            return
        elif args.config_cmd == 'validate':
            is_valid, message = config.validate_api_key()
            if is_valid:
                ui.print_success(f"[OK] {message}")
            else:
                ui.print_error(f"[ERROR] {message}")
            return
        else:
            parser_config.print_help()
            return

        # Legacy flags (kept for compatibility)
        if getattr(args, 'set', None):
            config.set_api_key(args.set)
            return
        if getattr(args, 'show', False):
            config.show_api_key()
            return
        if getattr(args, 'remove', False):
            config.remove_api_key()
            return
    # Default: start agent
    # Check API key before starting
    if not config.is_configured():
        ui.print_error("[ERROR] No API key configured.")
        ui.print_info("Use 'pai config set <API_KEY>' to set your Google Gemini API key.")
        return 1

    # Configure LLM runtime if flags provided
    model = getattr(args, 'model', None)
    temperature = getattr(args, 'temperature', None)
    if model is not None or temperature is not None:
        llm.set_runtime_model(model, temperature)

    try:
        agent.start_interactive_session()
    except KeyboardInterrupt:
        ui.print_info("\nSession terminated by user.")
    except Exception as e:
        ui.print_error(f"An error occurred during the session: {e}")
        return 1

if __name__ == "__main__":
    main()
\end{lstlisting}

\subsection*{ui.py}
\begin{lstlisting}[language=Python, captionpos=b, caption={Modul ui.py (lengkap, ASCII-only).}, label={appA:ui},
  literate={✓}{}1 {✗}{}1 {→}{}1 {“}{{"}}1 {”}{{"}}1 {‘}{{'}}1 {’}{{'}}1 {‎}{}1 {‏}{}1 {‍}{}1 {️}{}1]
# paicode/ui.py

from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.theme import Theme
from rich.rule import Rule
from rich.box import ROUNDED
from rich.text import Text

# Define a custom theme for consistency
custom_theme = Theme({
    "info": "dim default",
    "success": "bold green",
    "warning": "yellow",
    "error": "bold red",
    "action": "bold bright_blue", 
    "plan": "default", 
    "path": "underline italic bright_blue"
})

# Create a single console instance to be used across the application
console = Console(theme=custom_theme)

def print_success(message: str):
    """Displays a success message with a checkmark icon."""
    console.print(f"[success][OK] {message}[/success]")

def print_error(message: str):
    """Displays an error message with a cross icon."""
    console.print(f"[error][ERROR] {message}[/error]")

def print_warning(message: str):
    """Displays a warning message."""
    console.print(f"[warning]! {message}[/warning]")

def print_info(message: str):
    """Displays an informational message."""
    console.print(f"[info]i {message}[/info]")
    
def print_action(message: str):
    """Displays an action being performed by the agent."""
    console.print(f"[action]-> {message}[/action]")

def display_panel(content: str, title: str, language: str = None):
    """Displays content within a panel, with optional syntax highlighting."""
    if language:
        # Use Syntax for code highlighting
        display_content = Syntax(content, language, theme="monokai", line_numbers=True)
    else:
        display_content = content
    
    console.print(Panel(display_content, title=f"[bold grey50]{title}[/bold grey50]", border_style="grey50", expand=False))

def print_rule(title: str):
    """Displays a horizontal rule with a title."""
    console.print(Rule(f"[bold]{title}[/bold]", style="grey50"))
\end{lstlisting}

\subsection*{llm.py}
\begin{lstlisting}[language=Python, captionpos=b, caption={Modul llm.py (lengkap, ASCII-only).}, label={appA:llm},
  literate={✗}{}1 {→}{}1 {“}{{"}}1 {”}{{"}}1 {‘}{{'}}1 {’}{{'}}1 {‎}{}1 {‏}{}1 {‍}{}1 {️}{}1]
import os
import warnings
import time

# Reduce noisy STDERR logs from gRPC/absl before importing Google SDKs.
# These settings aim to suppress INFO/WARNING/ERROR logs emitted by native libs
# that happen prior to Python log initialization.
os.environ.setdefault("GRPC_VERBOSITY", "NONE")
os.environ.setdefault("GRPC_LOG_SEVERITY", "ERROR")
# Abseil logging (used by some Google native deps). 3 ~ FATAL-only
os.environ.setdefault("ABSL_LOGGING_MIN_LOG_LEVEL", "3")
# glog compatibility (some builds respect this env var)
os.environ.setdefault("GLOG_minloglevel", "3")
# Additional environment variables to suppress Google SDK warnings
os.environ.setdefault("GOOGLE_CLOUD_DISABLE_GRPC", "true")
os.environ.setdefault("GRPC_ENABLE_FORK_SUPPORT", "false")

# Suppress specific warnings
warnings.filterwarnings("ignore", category=UserWarning, module="google")
warnings.filterwarnings("ignore", message=".*ALTS.*")
warnings.filterwarnings("ignore", message=".*log messages before absl::InitializeLog.*")

import google.generativeai as genai
from . import config, ui

DEFAULT_MODEL = os.getenv("PAI_MODEL", "gemini-2.5-flash-lite")
try:
    DEFAULT_TEMPERATURE = float(os.getenv("PAI_TEMPERATURE", "0.3"))
    # Clamp temperature to safe range
    if DEFAULT_TEMPERATURE < 0.0:
        DEFAULT_TEMPERATURE = 0.0
    elif DEFAULT_TEMPERATURE > 2.0:
        DEFAULT_TEMPERATURE = 2.0
except ValueError:
    DEFAULT_TEMPERATURE = 0.3

# Global model holder
model = None
_runtime = {
    "name": None,
    "temperature": None,
}

def set_runtime_model(model_name: str | None = None, temperature: float | None = None):
    """Set the runtime model configuration."""
    global model, _runtime
    
    # Update runtime settings
    if model_name is not None:
        _runtime["name"] = model_name
    if temperature is not None:
        temperature = max(0.0, min(2.0, temperature))
        _runtime["temperature"] = temperature
    
    # Reset model so it gets recreated with new settings on next use
    model = None

# Initialize runtime settings (model will be created when needed)
_runtime = {
    "name": DEFAULT_MODEL,
    "temperature": DEFAULT_TEMPERATURE
}

def _prepare_runtime() -> bool:
    """Configure API key and ensure model object exists.
    
    Returns:
        bool: True if successful, False otherwise.
    """
    global model
    
    # Get single API key
    api_key = config.get_api_key()
    
    if not api_key:
        ui.print_error("Error: No API key configured. Use 'pai config set <API_KEY>'.")
        model = None
        return False
    
    try:
        genai.configure(api_key=api_key)
        if model is None:
            # Build model using stored runtime prefs
            name = _runtime.get("name") or DEFAULT_MODEL
            temp = _runtime.get("temperature") if _runtime.get("temperature") is not None else DEFAULT_TEMPERATURE
            generation_config = {"temperature": temp}
            model = genai.GenerativeModel(name, generation_config=generation_config)
        return True
    except Exception as e:
        ui.print_error(f"Failed to configure API key: {e}")
        model = None
        return False

def _is_rate_limit_error(error: Exception) -> bool:
    """Detect if an exception is a rate limit error.
    
    Args:
        error: The exception to check
        
    Returns:
        True if it's a rate limit error, False otherwise
    """
    error_msg = str(error).lower()
    
    # Common rate limit indicators
    rate_limit_keywords = [
        'rate limit', 'rate_limit', 'ratelimit',
        'quota', 'quota exceeded',
        'resource exhausted', 'resourceexhausted',
        '429', 'too many requests',
        'limit exceeded', 'requests per minute'
    ]
    
    return any(keyword in error_msg for keyword in rate_limit_keywords)

def _clean_response_text(text: str) -> str:
    """Clean markdown artifacts from LLM response.
    
    Args:
        text: Raw response text from LLM
        
    Returns:
        Cleaned text without markdown code blocks
    """
    cleaned_text = text.strip()
    
    # Remove all common markdown code block patterns
    code_block_prefixes = [
        "```python", "```html", "```css", "```javascript", "```js",
        "```typescript", "```ts", "```json", "```yaml", "```yml",
        "```bash", "```sh", "```diff", "```xml", "```sql",
        "```java", "```cpp", "```c", "```go", "```rust", "```ruby",
        "```php", "```markdown", "```md", "```text", "```txt", "```"
    ]
    
    for prefix in code_block_prefixes:
        if cleaned_text.startswith(prefix):
            cleaned_text = cleaned_text[len(prefix):].strip()
            break
    
    # Remove trailing code block markers
    if cleaned_text.endswith("```"):
        cleaned_text = cleaned_text[:-len("```")].strip()
    
    # Remove any remaining language tags at the start
    lines = cleaned_text.split('\n')
    if lines and len(lines[0].strip()) < 20 and lines[0].strip().lower() in [
        'html', 'css', 'javascript', 'js', 'python', 'json', 'yaml', 
        'bash', 'sh', 'diff', 'xml', 'sql', 'java', 'cpp', 'c', 'go', 
        'rust', 'ruby', 'php', 'markdown', 'md', 'text', 'txt', 'on'
    ]:
        cleaned_text = '\n'.join(lines[1:]).strip()
    
    return cleaned_text

def generate_text(prompt: str, call_purpose: str = "thinking") -> str:
    """
    Generate text with single API key - optimized for 2-call system.
    
    Args:
        prompt: The prompt to send to the LLM
        call_purpose: Purpose of the call for logging (e.g., "planning", "execution")
        
    Returns:
        The cleaned response text, or empty string if failed
    """
    global model
    
    # Ensure model is configured
    if model is None:
        if not _prepare_runtime():
            return ""
    
    try:
        # Show status with purpose
        status_msg = f"[bold yellow]Agent {call_purpose}..."
        
        with ui.console.status(status_msg, spinner="dots"):
            response = model.generate_content(prompt)
        
        # Success! Clean and return the response
        cleaned_text = _clean_response_text(response.text)
        
        # Log token usage if available (for optimization)
        if hasattr(response, 'usage_metadata'):
            usage = response.usage_metadata
            ui.print_info(f"Tokens: {usage.prompt_token_count} -> {usage.candidates_token_count}")
        
        return cleaned_text
        
    except Exception as e:
        is_rate_limit = _is_rate_limit_error(e)
        
        if is_rate_limit:
            ui.print_error("[ERROR] Rate limit reached. Please wait a few minutes before trying again.")
            ui.print_info("Consider using a different API key if available.")
        else:
            ui.print_error(f"[ERROR] LLM API error: {e}")
        
        return ""

def test_api_connection() -> bool:
    """Test if API connection works."""
    test_response = generate_text("Say 'Hello' if you can hear me.", "connection test")
    return len(test_response) > 0
\end{lstlisting}

\subsection*{\texttt{\_\_init\_\_.py}}
\begin{lstlisting}[language=Python, captionpos=b, caption={Modul \_\_init\_\_.py (paicode package), ASCII-only.}, label={appA:init}]
"""Pai Code package.

This package provides a command-line based agentic AI for software development.
"""

__all__ = [
    # Public modules
    "agent",
    "cli",
    "config",
    "llm",
    "ui",
    "workspace",
]

__version__ = "0.1.0"
\end{lstlisting}

\subsection*{requirements.txt}
\begin{lstlisting}[captionpos=b, caption={File requirements.txt}, label={appA:requirements}]
google-generativeai>=0.5.4
python-dotenv>=1.0.1
rich>=13.7.1
Pygments>=2.16.0
prompt_toolkit>=3.0.43
\end{lstlisting}

\subsection*{setup.py}
\begin{lstlisting}[language=Python, captionpos=b, caption={File setup.py (konfigurasi setuptools)}, label={appA:setup-py}]
from setuptools import setup

if __name__ == "__main__":
    setup()
\end{lstlisting}

\subsection*{setup.cfg}
\begin{lstlisting}[captionpos=b, caption={File setup.cfg (metadata dan konfigurasi paket)}, label={appA:setup-cfg}]
[metadata]
name = pai-code
version = 0.1.0
description = A command-line based agentic AI for software development.
long_description = file: README.md
long_description_content_type = text/markdown
author = gtkrshnaaa
author_email = gtkrshnaaa@email.com
license = MIT
license_files = LICENSE

[options]
packages = find:
python_requires = >=3.10
install_requires =
    google-generativeai>=0.5.4
    python-dotenv>=1.0.1
    rich>=13.7.1
    Pygments>=2.16.0
include_package_data = True

[options.packages.find]
where = .

[options.entry_points]
console_scripts =
    pai = paicode.cli:main
\end{lstlisting}

\subsection*{pyproject.toml}
\begin{lstlisting}[captionpos=b, caption={File pyproject.toml (konfigurasi build system)}, label={appA:pyproject}]
# pyproject.toml

[build-system]
requires = ["setuptools>=61", "wheel"]
build-backend = "setuptools.build_meta"
\end{lstlisting}

\subsection*{makefile}
\begin{lstlisting}[language=make, captionpos=b, caption={File makefile (task automation untuk development dan deployment)}, label={appA:makefile}]
.PHONY: run export-all install venv-activate setup install-cli uninstall-cli

install:
	@if [ ! -d .venv ]; then \
		echo "[install] Creating virtual environment at .venv"; \
		python3 -m venv .venv; \
	else \
		echo "[install] Reusing existing virtual environment at .venv"; \
	fi
	. .venv/bin/activate; python -m pip install --upgrade pip setuptools wheel
	. .venv/bin/activate; pip install -r requirements.txt
	. .venv/bin/activate; pip install -e .

run:
	. .venv/bin/activate; python -m paicode.cli auto

export-all:
	@mkdir -p z_project_list
	@echo "Exporting project files to z_project_list/listing.txt..."
	@rm -f z_project_list/listing.txt
	@for f in $$(find . -type f \
		-not -path '*/\.*' \
		-not -path '*/__pycache__/*' \
		-not -path '*.egg-info/*' \
		-not -path './z_project_list/*' \
		-not -name ".gitkeep" \
		| sort); do \
			echo "=== $$f ===" >> z_project_list/listing.txt; \
			cat $$f >> z_project_list/listing.txt; \
			echo "\n" >> z_project_list/listing.txt; \
	done
	@echo "Export complete."

venv-activate:
	@echo "To activate the virtual environment, run:"
	@echo "  source .venv/bin/activate"

setup: install install-cli
	@echo "Pai CLI installed. Ensure $$HOME/.local/bin is in your PATH, then run: pai"

install-cli:
	@mkdir -p $(HOME)/.local/bin
	@echo "Installing launcher to $(HOME)/.local/bin/pai"
	@echo '#!/usr/bin/env bash' > $(HOME)/.local/bin/pai
	@echo '# Suppress noisy gRPC/absl logs' >> $(HOME)/.local/bin/pai
	@echo 'export GRPC_VERBOSITY="NONE"' >> $(HOME)/.local/bin/pai
	@echo 'export GRPC_LOG_SEVERITY="ERROR"' >> $(HOME)/.local/bin/pai
	@echo 'export ABSL_LOGGING_MIN_LOG_LEVEL="3"' >> $(HOME)/.local/bin/pai
	@echo 'export GLOG_minloglevel="3"' >> $(HOME)/.local/bin/pai
	@echo 'export GOOGLE_CLOUD_DISABLE_GRPC="true"' >> $(HOME)/.local/bin/pai
	@echo 'export GRPC_ENABLE_FORK_SUPPORT="false"' >> $(HOME)/.local/bin/pai
	@echo 'SCRIPT_DIR="$$(cd "$$(dirname "$${BASH_SOURCE[0]}")" && pwd)"' >> $(HOME)/.local/bin/pai
	@echo 'APPDIR="$(shell pwd)"' >> $(HOME)/.local/bin/pai
	@echo 'VENVDIR="$$APPDIR/.venv"' >> $(HOME)/.local/bin/pai
	@echo 'PY="$$VENVDIR/bin/python"' >> $(HOME)/.local/bin/pai
	@echo '# Redirect stderr to suppress remaining warnings' >> $(HOME)/.local/bin/pai
	@echo 'if [ -x "$$VENVDIR/bin/pai" ]; then' >> $(HOME)/.local/bin/pai
	@echo '  exec "$$VENVDIR/bin/pai" "$$@" 2>/dev/null' >> $(HOME)/.local/bin/pai
	@echo 'elif [ -x "$$PY" ]; then' >> $(HOME)/.local/bin/pai
	@echo '  exec "$$PY" -m paicode.cli "$$@" 2>/dev/null' >> $(HOME)/.local/bin/pai
	@echo 'else' >> $(HOME)/.local/bin/pai
	@echo '  exec python3 -m paicode.cli "$$@" 2>/dev/null' >> $(HOME)/.local/bin/pai
	@echo 'fi' >> $(HOME)/.local/bin/pai
	@chmod +x $(HOME)/.local/bin/pai
	@# Ensure ~/.local/bin is in PATH (append to ~/.bashrc if missing)
	@if [ -f $(HOME)/.bashrc ]; then \
		grep -qxF 'export PATH="$$HOME/.local/bin:$$PATH"' $(HOME)/.bashrc || printf '\n# Added by pai install-cli\nexport PATH="$$HOME/.local/bin:$$PATH"\n' >> $(HOME)/.bashrc; \
	fi
	@echo "Ensured PATH includes $$HOME/.local/bin in $$HOME/.bashrc. Run: 'source $$HOME/.bashrc' or open a new terminal."
	@echo "Done. Ensure $(HOME)/.local/bin is in your PATH. Try running: pai --help"

uninstall-cli:
	@rm -f $(HOME)/.local/bin/pai
	@# Remove PATH line added by install-cli (safe if absent)
	@sed -i '/^# Added by pai install-cli$/d' $(HOME)/.bashrc || true
	@sed -i '/^export PATH="\$HOME\/\.local\/bin:\$PATH"$/d' $(HOME)/.bashrc || true
	@echo "Launcher removed: $(HOME)/.local/bin/pai"
\end{lstlisting}
