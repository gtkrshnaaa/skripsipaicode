% Bibliography entries (use BibTeX). Replace with your real references.
@inproceedings{brown2020gpt3,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and others},
  booktitle={NeurIPS},
  year={2020}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{gemini2023,
  title={Gemini: A Family of Highly Capable Multimodal Models},
  author={Anil, Rohan and Bai, Yuntao and Chen, Xinyun and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{llama2_2023,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={{Meta AI}},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{yao2023react,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and others},
  booktitle={ICLR},
  year={2023}
}

@article{schick2023toolformer,
  title={Toolformer: Language Models Can Teach Themselves to Use Tools},
  author={Schick, Timo and Sch{"u}tz, Jane and Dwivedi-Yu, Jane and others},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023}
}
